{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Análisis de Redes Sociales",
   "id": "b1ba8e7e4676e821"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.214590Z",
     "start_time": "2024-09-19T04:11:59.210274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode"
   ],
   "id": "5a5c7e9fd79694b4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.301955Z",
     "start_time": "2024-09-19T04:11:59.294606Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "id": "7aaf4cc50c1d5352",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\casti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Carga de Archivos",
   "id": "f55792fb295fc4f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.323550Z",
     "start_time": "2024-09-19T04:11:59.318963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_tweets(file_path):\n",
    "    tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-16') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                tweets.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    return pd.DataFrame(tweets)"
   ],
   "id": "cec5b8be967fb51d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Limpieza y Preprocesamiento de Datos",
   "id": "75c9a25c3f87b1b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.336175Z",
     "start_time": "2024-09-19T04:11:59.325583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_nested_json(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return json.loads(x.replace(\"'\", '\"'))\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    else:\n",
    "        return x"
   ],
   "id": "c0eaa6c72a84595d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.361117Z",
     "start_time": "2024-09-19T04:11:59.345194Z"
    }
   },
   "cell_type": "code",
   "source": "stop_words = set(stopwords.words('spanish'))",
   "id": "f97f3526ddc0f9ae",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definimos el preprocesamiento de texto",
   "id": "c809d531428f0aab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.382244Z",
     "start_time": "2024-09-19T04:11:59.377119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Convertir el texto a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Quitar URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Quitar caracteres especiales como `#`, `@` y apóstrofes\n",
    "    text = re.sub(r'[@#\\'’]', '', text)\n",
    "\n",
    "    # Quitar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Quitar signos de puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Normalizar caracteres acentuados\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # Quitar stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ],
   "id": "3a07c5a3bf59d47e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definimos el flujo de procesamiento de los archivos",
   "id": "717aa5eae6e31805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:11:59.412484Z",
     "start_time": "2024-09-19T04:11:59.406252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_file(file_path):\n",
    "    df = load_tweets(file_path)\n",
    "\n",
    "    # Convert the data in the columns from JSON strings to dictionaries\n",
    "    nested_columns = ['user', 'retweetedTweet', 'quotedTweet', 'mentionedUsers', 'inReplyToUser', 'media']\n",
    "    for column in nested_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].apply(parse_nested_json)\n",
    "\n",
    "    # Convert the date column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Preprocess text\n",
    "    df['processed_text'] = df['rawContent'].apply(preprocess_text)\n",
    "\n",
    "    # Extract mentioned users\n",
    "    df['mentioned_users'] = df['mentionedUsers'].apply(lambda users: [user['username'] for user in users])\n",
    "\n",
    "    # Identify if it is a retweet or reply and the interactions\n",
    "    df['is_retweet'] = df['retweetedTweet'].apply(lambda x: pd.notnull(x))\n",
    "\n",
    "    # Identify if it is a reply\n",
    "    df['is_reply'] = df['inReplyToTweetId'].apply(lambda x: pd.notnull(x))\n",
    "\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "\n",
    "    # Normalize user names\n",
    "    df.loc[:, 'user_normalized'] = df['user'].apply(lambda x: x['username'].lower() if isinstance(x, dict) else x)\n",
    "    # Normalize mentioned users\n",
    "    df.loc[:, 'mentioned_users_normalized'] = df['mentioned_users'].apply(lambda users: [user.lower() for user in users])\n",
    "\n",
    "    return df"
   ],
   "id": "ffe6024c821f72b8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:02.072514Z",
     "start_time": "2024-09-19T04:11:59.468509Z"
    }
   },
   "cell_type": "code",
   "source": "df_trafico = process_file('data/traficogt.txt')",
   "id": "acb4553c4c68c61d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:03.072466Z",
     "start_time": "2024-09-19T04:12:02.073532Z"
    }
   },
   "cell_type": "code",
   "source": "df_arevalo = process_file('data/tioberny.txt')",
   "id": "e62b126dd0f82ee7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extraemos un grafo de interacciones",
   "id": "2d9a82fad4642ba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:03.078377Z",
     "start_time": "2024-09-19T04:12:03.073515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_interactions(df):\n",
    "    edges = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['mentioned_users_normalized']:\n",
    "            for mentioned in row['mentioned_users_normalized']:\n",
    "                edges.append((row['user_normalized'], mentioned, 'mention'))\n",
    "        if row['is_reply']:\n",
    "            edges.append((row['user_normalized'], row['inReplyToTweetId'], 'reply'))\n",
    "\n",
    "    edges_df = pd.DataFrame(edges, columns=['source', 'target', 'type'])\n",
    "    G = nx.from_pandas_edgelist(edges_df, source='source', target='target', edge_attr='type', create_using=nx.DiGraph())\n",
    "    return G"
   ],
   "id": "55b30d6494f76866",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:03.381444Z",
     "start_time": "2024-09-19T04:12:03.080377Z"
    }
   },
   "cell_type": "code",
   "source": "G_trafico = extract_interactions(df_trafico)",
   "id": "e3580df9146948fc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:03.675062Z",
     "start_time": "2024-09-19T04:12:03.382451Z"
    }
   },
   "cell_type": "code",
   "source": "G_arevalo = extract_interactions(df_arevalo)",
   "id": "786b59af3f47bb9c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El grafo de tráfico tiene 5011 nodos y 11231 aristas\n"
     ]
    }
   ],
   "execution_count": 24,
   "source": "print(f\"El grafo de tráfico tiene {G_trafico.number_of_nodes()} nodos y {G_trafico.number_of_edges()} aristas\")",
   "id": "28e286424f62b439"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:58.846148Z",
     "start_time": "2024-09-19T04:12:58.840037Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"El grafo de Arevalo tiene {G_arevalo.number_of_nodes()} nodos y {G_arevalo.number_of_edges()} aristas\")",
   "id": "6ecd3c74afed9a09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El grafo de Arevalo tiene 4172 nodos y 16397 aristas\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Análisis Exploratorio",
   "id": "a694e1357f1e0b1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:12:03.679076Z",
     "start_time": "2024-09-19T04:12:03.676069Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ed85e201db5b78ec",
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
